{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566f0e1-a6cd-47a8-a21d-eb6b3ea95764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __      __  _____    ___________________________  \n",
    "#/  \\    /  \\/  _  \\  /   _____/\\______   \\_____  \\ \n",
    "#\\   \\/\\/   /  /_\\  \\ \\_____  \\  |     ___/ _(__  < \n",
    "# \\        /    |    \\/        \\ |    |    /       \\\n",
    "#  \\__/\\  /\\____|__  /_______  / |____|   /______  /\n",
    "#       \\/         \\/        \\/                  \\/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7296f-af74-4540-923d-8f339eaca4c8",
   "metadata": {},
   "source": [
    "# LFQ DIA Peptide-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60391b-daa9-47ce-954b-556d20fb1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "_PATH_ = \"\"\n",
    "INPUT_PATH = f\"{_PATH_}lfq.dia.peptides.csv\"                   \n",
    "METADATA_PATH = f\"{_PATH_}sample_metadata.csv\"                 # Must contain 'sample_id', 'group', 'label'\n",
    "OUTPUT_FILTERED = f\"{_PATH_}filtered_peptide_matrix.csv\"\n",
    "OUTPUT_AGGREGATED = f\"{_PATH_}aggregated_protein_matrix.csv\"\n",
    "OUTPUT_RLE = f\"{_PATH_}RLE.png\"\n",
    "OUTPUT_IMPUTE = f\"{_PATH_}Post-imputation_histogram.png\"\n",
    "OUTPUT_PCA = f\"{_PATH_}PCA.svg\"\n",
    "OUTPUT_UMAP2D = f\"{_PATH_}UMAP-2D.svg\"\n",
    "OUTPUT_UMAP3D = f\"{_PATH_}UMAP-3D.svg\"\n",
    "\n",
    "\n",
    "MIN_COMPLETENESS = 0.5  # Intragroup completeness threshold\n",
    "CV_THRESHOLD = 3.5      # Intragroup CV upper limit\n",
    "TOP_N = 3               # Top-N peptides per protein for aggregation\n",
    "MIN_INTENSITY_Q = 0.10  # Minimum acceptable median intensity (percentile)\n",
    "PSEUDOCOUNT = 1e-5      # For log-transform and imputation\n",
    "N_GROUP = 2             # Number of groups\n",
    "QUANTILE = 0.10         # Quantile to sample skewnormal imputation\n",
    "SHIFT = 2.00            # Number of std deviations to shift the gaussian center to the left\n",
    "WIDTH = 0.33            # Std deviation multiplier (more of less variance)\n",
    "MIN_ALLOWED = None      # ABS minimum for imputed values\n",
    "SEED = 42               # Raondom seed for reproducibility\n",
    "LEGEND_LAB = ''\n",
    "LEGEND_LAB1 = ''\n",
    "LEGEND_LAB2 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903719b-f19f-428e-b2d5-8b1099d160b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"n_jobs value 1 overridden to 1\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f32f99-c941-4d71-bfe6-1997075c648e",
   "metadata": {},
   "source": [
    "## Normalization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742c6d1-96a6-42f5-afcc-29fafc28b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Method 1: RobNorm-like (robust median centering) -------------------\n",
    "def robnorm_normalize(df):\n",
    "    df_norm = df.copy()\n",
    "    for col in df.columns:\n",
    "        center = np.median(df[col].dropna())\n",
    "        df_norm[col] = df[col] - center\n",
    "    return df_norm\n",
    "\n",
    "# ---------------------- Method 2: TMM-like Scaling ----------------------------------\n",
    "def tmm_normalize(df):\n",
    "    df = df.copy()\n",
    "    ref_sample = df.median(axis=1)  # pseudo-reference sample\n",
    "\n",
    "    scaling_factors = []\n",
    "    for col in df.columns:\n",
    "        ratios = df[col] - ref_sample\n",
    "        trimmed = ratios[(ratios > np.percentile(ratios, 5)) & (ratios < np.percentile(ratios, 95))]\n",
    "        scaling_factors.append(trimmed.median())\n",
    "\n",
    "    scale_series = pd.Series(scaling_factors, index=df.columns)\n",
    "    return df - scale_series  # subtract log-scale factor per sample\n",
    "\n",
    "\n",
    "#---------------------- Method 3: Variance Stabilizing Normalization -------------------\n",
    "def vsn_normalize(df):\n",
    "    df_trans = df.copy().apply(lambda x: 2**x)\n",
    "    offset = np.median(df.values[np.isfinite(df.values)])\n",
    "\n",
    "    # Apply transformation: log2(a + sqrt(x + offset))\n",
    "    vsn_func = lambda x: np.log2(1.0 + np.sqrt(x + offset))\n",
    "    df_vsn = df_trans.applymap(vsn_func)\n",
    "\n",
    "    df_vsn = (df_vsn - df_vsn.mean()) / df_vsn.std()\n",
    "\n",
    "    return df_vsn\n",
    "\n",
    "#---------------------- Method 4: RobNorm-Like with MAD Scaling -----------------------\n",
    "# Function for robust normalization\n",
    "def robMAD_normalize(df_log2, preserve_index=True):\n",
    "    normalized_df = df_log2.copy()\n",
    "    for col in df_log2.columns:\n",
    "        values = df_log2[col]\n",
    "        center = np.median(values.dropna())\n",
    "        scale = median_abs_deviation(values.dropna())\n",
    "        # Avoid division by zero\n",
    "        scale = scale if scale != 0 else 1.0\n",
    "        normalized = (values - center) / scale\n",
    "\n",
    "        normalized_df[col] = normalized\n",
    "\n",
    "    if preserve_index:\n",
    "        normalized_df.index = df_log2.index\n",
    "\n",
    "    return normalized_d\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f994a0-b2bb-4dfe-b45e-dfdc6fc70580",
   "metadata": {},
   "source": [
    "## Imputation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d528a-f441-4e66-bcbd-de863899ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Method 1: Left-Tail Skewnorm -------------------\n",
    "def left_tail_skewnorm_impute(df, quantile=QUANTILE, seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    imputed_df = df.copy()\n",
    "\n",
    "    for col in imputed_df.columns:\n",
    "        observed = imputed_df[col].dropna()\n",
    "        \n",
    "        # Fit skew-normal: returns shape (a), location (loc), scale (scale)\n",
    "        s, loc, scale = skewnorm.fit(observed)\n",
    "\n",
    "        # Sample missing values from the left tail\n",
    "        missing_mask = imputed_df[col].isna()\n",
    "        n_missing = missing_mask.sum()\n",
    "        if n_missing == 0:\n",
    "            continue\n",
    "        \n",
    "        # Define upper bound in left tail: e.g., 10th percentile in the fitted skewnorm\n",
    "        left_tail_max = skewnorm.ppf(quantile, s, loc, scale)\n",
    "        # Impute from skewnorm, truncated to only accept samples <= left_tail_max\n",
    "        imputed_vals = []\n",
    "        while len(imputed_vals) < n_missing:\n",
    "            sample = skewnorm.rvs(s, loc, scale, size=(n_missing - len(imputed_vals)))\n",
    "            tail_samples = sample[sample <= left_tail_max]\n",
    "            imputed_vals.extend(tail_samples)\n",
    "        imputed_vals = np.array(imputed_vals)[:n_missing]\n",
    "        imputed_df.loc[missing_mask, col] = imputed_vals\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "# ---------------------- Method 2: Left-Tail Gaussian -------------------\n",
    "def left_shifted_gaussian_impute(df, shift=SHIFT, width=WIDTH, min_allowed=MIN_ALLOWED, seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    imputed_df = df.copy()\n",
    "\n",
    "    for col in imputed_df.columns:\n",
    "        col_data = imputed_df[col]\n",
    "        missing_mask = col_data.isna()\n",
    "        if missing_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        observed_vals = col_data[~missing_mask]\n",
    "        mean_obs = observed_vals.mean()\n",
    "        std_obs = observed_vals.std()\n",
    "\n",
    "        low_gaussian = np.random.normal(loc=mean_obs - shift * std_obs,\n",
    "                                        scale=width * std_obs,\n",
    "                                        size=missing_mask.sum())\n",
    "\n",
    "        if min_allowed is not None:\n",
    "            low_gaussian = np.clip(low_gaussian, min_allowed, None)\n",
    "\n",
    "        # Impute\n",
    "        imputed_df.loc[missing_mask, col] = low_gaussian\n",
    "\n",
    "    return imputed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe1323-fc16-4150-98f6-6f6205c605ec",
   "metadata": {},
   "source": [
    "## Aggregation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a2ed4-5d96-4996-8600-269f1442f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Top-N Mean ------------------------\n",
    "def aggregate_top_n(df, topn=TOP_N):\n",
    "    def topn_agg(protein_df):\n",
    "        peptide_medians = protein_df.median(axis=1)\n",
    "        top_peptides = peptide_medians.nlargest(topn).index\n",
    "        return protein_df.loc[top_peptides].mean(axis=0)\n",
    "    aggdf = (\n",
    "        df\n",
    "        .groupby(level='Accession', group_keys=False)\n",
    "        .apply(topn_agg)\n",
    "    )\n",
    "    return aggdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c538fde-ef23-49d2-bc6b-12722dcbe7c9",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db624e-8722-4fc7-a86e-e62d9beb13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load data\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "meta_df = pd.read_csv(METADATA_PATH)\n",
    "sample_ids = meta_df['sample_id'].values\n",
    "sample_labels = meta_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daac031-ccb3-4aee-85f1-20b489a0ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wrangle dfs and check for outliers\n",
    "peptide_df = df.copy()\n",
    "prefixes = ['Accession', 'Peptide', 'Area']\n",
    "selected_columns2 = df.columns[[any(prefix in col for prefix in prefixes) for col in df.columns]] # select sample id columns and accession, peptide\n",
    "peptide_df = peptide_df.loc[:, selected_columns2]\n",
    "peptide_df = peptide_df.dropna(subset=['Accession']) # remove NaN\n",
    "peptide_df = peptide_df[~peptide_df.Accession.str.contains('#CONTAM#')] # Remove contaminants\n",
    "peptide_df['Accession'] = peptide_df.Accession.str.split('|').str[0] # Split Accession string to aquire Uniprot Accession ID\n",
    "peptide_df.set_index(['Accession', 'Peptide'], inplace=True)\n",
    "peptide_df = peptide_df.iloc[:, :-(N_GROUP+1)]\n",
    "peptide_df = peptide_df[sample_ids]  # Ensure correct column order\n",
    "\n",
    "group_map = meta_df.set_index('sample_id')['group'].to_dict()\n",
    "grouped_samples = meta_df.groupby('group')['sample_id'].apply(list).to_dict() # Group metadata\n",
    "\n",
    "log_data = np.log(peptide_df + 0.00001) # RLE \n",
    "peptide_df = peptide_df.replace(0, np.nan)\n",
    "dfboxen = log_data.T.apply(lambda x: x-np.nanmedian(x))\n",
    "\n",
    "# Plot\n",
    "fig = sns.boxenplot(\n",
    "    data=dfboxen.iloc[:,1:].T,\n",
    "    orient='h',\n",
    "    width_method=\"linear\")\n",
    "plt.xlabel('$Log10$ Relative Abundance')\n",
    "plt.ylabel('Sample')\n",
    "fig.set_yticks(sample_ids)\n",
    "fig.set_yticklabels(sample_labels)\n",
    "plt.title('RLE $Pre$-Normalization')\n",
    "plt.savefig(OUTPUT_RLE)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2df4fe-7d3a-4c60-af9b-3735e06eed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filter for completeness\n",
    "def intragroup_completeness(row):\n",
    "    for group, samples in grouped_samples.items():\n",
    "        completeness = row[samples].count() / len(samples)\n",
    "        if completeness > MIN_COMPLETENESS:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "mask_complete = peptide_df.apply(intragroup_completeness, axis=1)\n",
    "peptide_df = peptide_df[mask_complete]\n",
    "\n",
    "# 3. Log2 transform\n",
    "peptide_df_log = np.log2(peptide_df + PSEUDOCOUNT)\n",
    "\n",
    "# Step 4: QRILC-style impute\n",
    "log2_imputed = left_tail_skewnorm_impute(peptide_df_log, quantile=QUANTILE, seed=SEED) # Adjust as neccessary\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(log2_imputed.values.flatten(), bins=80, color='slateblue')\n",
    "plt.xlabel(\"Log2 Intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Protein Intensities After $Skewnorm$ Left-Tail Imputation\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_IMPUTE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f2584-df4e-47ba-8069-e8b6df58fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension Reduction of Prenormalized data with PCA\n",
    "X0 = log2_imputed.T\n",
    "pca = PCA(n_components=min(X0.shape))\n",
    "components = pca.fit_transform(X0)\n",
    "\n",
    "fig = px.scatter(pd.DataFrame(components).rename(columns={0: 'PC 1', 1: 'PC 2'}), x='PC 1', y='PC 2',\n",
    "                 color=meta_df['group'],\n",
    "                 template='plotly_white',\n",
    "                 labels={\n",
    "                 'PC 1': f'PC 1: {round(pca.explained_variance_ratio_[0] * 100, 1)}%',\n",
    "                 'PC 2': f'PC 2: {round(pca.explained_variance_ratio_[1] * 100, 1)}%',\n",
    "                 'color': LEGEND_LAB},\n",
    "                 text=sample_labels)\n",
    "\n",
    "fig.data[0].name = LEGEND_LAB1\n",
    "fig.data[1].name = LEGEND_LAB2\n",
    "\n",
    "fig.update_layout(font=dict(family='Arial', size=14), legend=dict(\n",
    "    orientation='h',\n",
    "    yanchor='bottom',\n",
    "    y=1.01,\n",
    "    xanchor='center',\n",
    "    x=0.5,\n",
    "    font=dict(family='Arial', size=14)\n",
    "))\n",
    "\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_traces(marker_size=10)\n",
    "fig.write_image(OUTPUT_PCA)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5948b8-35f3-4cc0-b431-3b710191f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Trimmed Means M-values (Normalization)\n",
    "peptide_df_norm = tmm_normalize(log2_imputed)\n",
    "\n",
    "# 6. Filter by peptide intensity\n",
    "medians = peptide_df_norm.median(axis=1)\n",
    "intensity_threshold = medians.quantile(MIN_INTENSITY_Q)\n",
    "mask_intensity = medians >= intensity_threshold\n",
    "peptide_df_norm = peptide_df_norm[mask_intensity]\n",
    "\n",
    "# 7. Filter by intragroup CV\n",
    "def intragroup_cv_ok(row):\n",
    "    for group, samples in grouped_samples.items():\n",
    "        values = row[samples]\n",
    "        if values.mean() == 0:\n",
    "            return False\n",
    "        cv = values.std() / values.mean()\n",
    "        if cv > CV_THRESHOLD or np.isnan(cv):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "mask_cv = peptide_df_norm.apply(intragroup_cv_ok, axis=1)\n",
    "peptide_df_filtered = peptide_df_norm[mask_cv]\n",
    "peptide_df_filtered.to_csv(OUTPUT_FILTERED)\n",
    "\n",
    "# 8. Aggregate by Top-N most intense peptides\n",
    "protein_df = aggregate_top_n(peptide_df_filtered, topn=TOP_N)\n",
    "\n",
    "# Export for limma and report proteins aggregated/peptides retained\n",
    "protein_df.to_csv(OUTPUT_AGGREGATED)\n",
    "print(f\"Retained peptides: {peptide_df_filtered.shape[0]}\")\n",
    "print(f\"Aggregated proteins: {protein_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c29ec-f056-4a46-869f-3571e561ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension Reduction\n",
    "X = protein_df.T\n",
    "y = meta_df.group\n",
    "\n",
    "umap_2d = UMAP(n_components=2, n_neighbors=X.shape[0]//N_GROUP, random_state=42)\n",
    "umap_3d = UMAP(n_components=3, n_neighbors=X.shape[0]//N_GROUP, random_state=42)\n",
    "\n",
    "proj_2d = umap_2d.fit_transform(X)\n",
    "proj_3d = umap_3d.fit_transform(X)\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    pd.DataFrame(proj_2d).rename(columns={0: 'UMAP1', 1: 'UMAP2'}),\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color=y,\n",
    "    template='plotly_white',\n",
    "    labels={'color': LEGEND_LAB},\n",
    "    text=sample_labels\n",
    ")\n",
    "fig_2d.data[0].name = LEGEND_LAB1\n",
    "fig_2d.data[1].name = LEGEND_LAB2\n",
    "\n",
    "fig_2d.update_layout(font=dict(family='Arial', size=14), legend=dict(\n",
    "    orientation='h',\n",
    "    yanchor='bottom',\n",
    "    y=1.01,\n",
    "    xanchor='center',\n",
    "    x=0.5,\n",
    "    font=dict(family='Arial', size=14)\n",
    "))\n",
    "\n",
    "fig_2d.update_traces(textposition='top center')\n",
    "fig_2d.update_traces(marker_size=10)\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    pd.DataFrame(proj_3d).rename(columns={0: 'UMAP1', 1: 'UMAP2', 2: 'UMAP3'}),\n",
    "    x='UMAP1', y='UMAP2', z='UMAP3',\n",
    "    color=y,\n",
    "    labels={'color': LEGEND_LAB},\n",
    "    #text=X.index\n",
    ")\n",
    "\n",
    "fig_3d.update_traces(textposition='top center')\n",
    "fig_3d.update_traces(marker_size=6)\n",
    "\n",
    "fig_2d.write_image(OUTPUT_UMAP2D)\n",
    "fig_2d.show()\n",
    "fig_3d.write_image(OUTPUT_UMAP3D)\n",
    "fig_3d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
